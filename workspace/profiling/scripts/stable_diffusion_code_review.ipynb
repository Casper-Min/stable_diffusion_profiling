{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sympy import false\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers.utils import randn_tensor\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         \"-m\",\n",
    "#         \"--model_id\",\n",
    "#         type=str,\n",
    "#         default=\"runwayml/stable-diffusion-v1-5\",\n",
    "#         help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-p\",\n",
    "#         \"--prompt\",\n",
    "#         type=str,\n",
    "#         default=\"a photograph of an astronaut riding a horse\",\n",
    "#         help=\"Text used to generate images.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-n\",\n",
    "#         \"--images_num\",\n",
    "#         type=int,\n",
    "#         default=1,\n",
    "#         help=\"How much images to generate.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-s\",\n",
    "#         \"--steps\",\n",
    "#         type=int,\n",
    "#         default=50,\n",
    "#         help=\"The number of denoising steps.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-wi\",\n",
    "#         \"--width\",\n",
    "#         type=int,\n",
    "#         default=512,\n",
    "#         help=\"The width in pixels of the generated image.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-he\",\n",
    "#         \"--height\",\n",
    "#         type=int,\n",
    "#         default=512,\n",
    "#         help=\"The height in pixels of the generated image.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-g\",\n",
    "#         \"--guidance\",\n",
    "#         type=float,\n",
    "#         default=7.5,\n",
    "#         help=\"Higher guidance scale encourages to generate images that are closely linked to the text.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-sd\",\n",
    "#         \"--seed\",\n",
    "#         type=int,\n",
    "#         default=42,\n",
    "#         help=\"Seed for random process.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-ci\",\n",
    "#         \"--cuda_id\",\n",
    "#         type=int,\n",
    "#         default=0,\n",
    "#         help=\"cuda_id.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-o\",\n",
    "#         \"--path\",\n",
    "#         type=str,\n",
    "#         default=\"outputs\",\n",
    "#         help=\"output path\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-pr\",\n",
    "#         \"--pre_defined_pipeline\",\n",
    "#         type=bool,\n",
    "#         default=False,\n",
    "#         help=\"Use pre-defined_pipeleine for custom_pipeline.\",\n",
    "#     )\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    if not len(imgs) == rows * cols:\n",
    "        raise ValueError(\"The specified number of rows and columns are not correct.\")\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure stable diffusion paramters\n",
    "# args = parse_args()\n",
    "# args.model\n",
    "\n",
    "from regex import B\n",
    "\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "a_height = 512\n",
    "a_width = 512\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "num_images_per_prompt = 1\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(930319)\n",
    "torch_device = torch.device(\"cuda\", 7)\n",
    "batch_size = len(prompt)\n",
    "with_predefined_pipeline = True\n",
    "output_path = \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == True:\n",
    "    \n",
    "    # 2. Construct pre-defined diffusion pipeline \n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "    \n",
    "    # 3. Load pre-defined diffusion pipeline to GPU\n",
    "    pipeline.to(torch_device)\n",
    "\n",
    "    # 2. Construct custom diffusion pipeline\n",
    "    # 2-1. Load the tokenizer and text encoder to tokenize and encode the text.\n",
    "    # text_encoder: Other diffusion models may use other encoders such as BERT(Default : CLIP)\n",
    "    # tokenizer: It must match the one used by the text_encoder model(Default : CLIPtokenizer)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "\n",
    "    # 2-2. The UNet model for generating the latents.\n",
    "    # unet: Model used to generate the latent representation of the input(Default : UNET))\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "    \n",
    "    # 2-3. The scheduler for denoising latent vector. (Deault : PNDM)\n",
    "    # scheduler: Scheduling algorithm used to progressively add noise to the image during training\n",
    "    scheduler = PNDMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    \n",
    "    # 2-4. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "    # vae: Autoencoder module used to decode latent representations into real images.\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "\n",
    "    # 3. Load custom diffusion pipeline to GPU\n",
    "    text_encoder.to(torch_device)\n",
    "    unet.to(torch_device)\n",
    "    vae.to(torch_device)\n",
    "\n",
    "elif with_predefined_pipeline == False:\n",
    "    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "    height = a_height or unet.config.sample_size * vae_scale_factor\n",
    "    width = a_width or unet.config.sample_size * vae_scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Predefined Pipeline\n",
    "if with_predefined_pipeline == True:\n",
    "    with torch.no_grad():\n",
    "        # 4~8. Execute Inference(With Predefined Pipeline)\n",
    "        height = a_height\n",
    "        width = a_width\n",
    "        image = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator).images\n",
    "        \n",
    "        grid = image_grid(image, rows=batch_size, cols=num_images_per_prompt)\n",
    "        grid.save(output_path+f\"/predefined_result_step_{num_inference_steps}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == False:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # 4. Tokenize the text and generate the embeddings from the prompt\n",
    "        # 4-1. generate conditional embeddings from text prompt\n",
    "        # 4-1-1. Generate token from prompt\n",
    "        text_input = tokenizer(prompt, padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        text_input_ids = text_input.input_ids\n",
    "\n",
    "        # 4-1-2. Generate embeddings from token\n",
    "        text_embeddings = text_encoder(text_input_ids.to(torch_device))[0]\n",
    "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "        text_embeddings = text_embeddings.to(dtype=text_encoder.dtype, device=torch_device)\n",
    "        bs_embed, seq_len, _ = text_embeddings.shape\n",
    "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # 4-2. get unconditional embeddings for classifier free guidance\n",
    "        # 4-2-1. Generate token from empty prompt\n",
    "        uncond_tokens = [\"\"]*batch_size\n",
    "        uncond_input = tokenizer(uncond_tokens, padding=\"max_length\",\n",
    "            max_length=text_embeddings.shape[1], truncation=True, return_tensors=\"pt\")\n",
    "        uncond_input_ids = uncond_input.input_ids\n",
    "\n",
    "        # 4-2-2. Generate unconditional embeddings from token\n",
    "        uncond_embeddings = text_encoder(uncond_input_ids.to(torch_device))[0]\n",
    "        # duplicate unconditional embeddings for each generation per prompt\n",
    "        seq_len = uncond_embeddings.shape[1]\n",
    "        uncond_embeddings = uncond_embeddings.to(dtype=text_encoder.dtype, device=torch_device)\n",
    "        uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # 4-3. concatenate the unconditional and text embeddings\n",
    "        # into a single batch to avoid doing two forward passes\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == False:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # 5. Initialize denoising network\n",
    "        # 5-1. Generate random noise\n",
    "        shape = (batch_size*num_images_per_prompt, unet.config.in_channels,\n",
    "                height//vae_scale_factor, width//vae_scale_factor)\n",
    "        latents = randn_tensor(shape, generator=generator, device=torch_device,\n",
    "                dtype=text_embeddings.dtype)\n",
    "        # 5-2. scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * scheduler.init_noise_sigma\n",
    "        # 5-3. initialize the scheduler with our chosen num_inference_steps\n",
    "        scheduler.set_timesteps(num_inference_steps-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == False:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # 6. denoising network loop(for num_inference_steps)\n",
    "        for t in tqdm(scheduler.timesteps):\n",
    "            # 6-1. expand the latents if we are doing classifier-free guidance\n",
    "            # to avoid doing two forward passes.            \n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "            # 6-2. UNET : predict the noise residual\n",
    "            noise_pred = unet(latent_model_input, timestep=t,\n",
    "                encoder_hidden_states=text_embeddings, return_dict=False)[0]\n",
    "            \n",
    "            # 6-3.reflect guidance scale on predicted noise to perform classifier-free guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # 6-4. subtract sampe x_(t) with predicted noise to generate sameple x_(t-1)\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == False:\n",
    "    # 7. Decode the image \n",
    "    # 7-1. scale the denoised latent by scaling factor required by the VAE\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    # 7-2. decode the image latents with vae\n",
    "    image = vae.decode(latents, return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_predefined_pipeline == False:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # 8. Post-process the image    \n",
    "        # convert the image to PIL and save it\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        # we always cast to float32 as this does not cause significant overhead\n",
    "        # and it is compatible with bfloat16\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        \n",
    "        grid = image_grid(pil_images, rows=1, cols=num_images_per_prompt)\n",
    "        grid.save(output_path+f\"/custom_result_step_{num_inference_steps}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_profile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
